=== Training Run 1 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 2 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 3 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 4 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 5 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 6 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 7 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 8 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 9 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
=== Training Run 10 ===
Training VAE...

Device: mps
Configuration:
latent_dim: 10
T: 1000
training:
  batch_size: 64
  epochs: 50
  binarized: true
  model_path: models/
optimizer:
  _target_: torch.optim.Adam
  lr: 0.001
models:
  model:
    _target_: models.vae.VAE
  name: vae
priors:
  prior:
    _target_: models.priors.VampPrior
    M: ${latent_dim}
    K: 10
    input_dim: 784
  name: vamp


Model architecture:
VAE(
  (prior): VampPrior(
    (encoder): GaussianEncoder(
      (encoder_net): Sequential(
        (0): Flatten(start_dim=1, end_dim=-1)
        (1): Linear(in_features=784, out_features=512, bias=True)
        (2): ReLU()
        (3): Linear(in_features=512, out_features=512, bias=True)
        (4): ReLU()
        (5): Linear(in_features=512, out_features=20, bias=True)
      )
    )
  )
  (decoder): BernoulliDecoder(
    (decoder_net): Sequential(
      (0): Linear(in_features=10, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=512, bias=True)
      (3): ReLU()
      (4): Linear(in_features=512, out_features=784, bias=True)
      (5): Unflatten(dim=-1, unflattened_size=(28, 28))
    )
  )
  (encoder): GaussianEncoder(
    (encoder_net): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=784, out_features=512, bias=True)
      (2): ReLU()
      (3): Linear(in_features=512, out_features=512, bias=True)
      (4): ReLU()
      (5): Linear(in_features=512, out_features=20, bias=True)
    )
  )
)

Training complete.
